{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/semih/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "import os\n",
    "import platform\n",
    "from subprocess import check_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var(name, shape, init=None, std=None):\n",
    "    if init is None:\n",
    "        if std is None:\n",
    "            std = (2./shape[0])**0.5\n",
    "        init = tf.truncated_normal_initializer(stddev=std)\n",
    "    return tf.get_variable(name=name, shape=shape, \n",
    "                           dtype=tf.float32, initializer=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo,  encoding='latin1')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tra_file_1 = './cifar-10-batches-py/data_batch_1'\n",
    "tra_file_2 = './cifar-10-batches-py/data_batch_2'\n",
    "tra_file_3 = './cifar-10-batches-py/data_batch_3'\n",
    "tra_file_4 = './cifar-10-batches-py/data_batch_4'\n",
    "tra_file_5 = './cifar-10-batches-py/data_batch_5'\n",
    "\n",
    "test_file = './cifar-10-batches-py/test_batch'\n",
    "\n",
    "x = None \n",
    "y = None \n",
    "\n",
    "for i in range(5):\n",
    "    if i == 0: \n",
    "        tra_batch = unpickle(tra_file_1)\n",
    "    elif i == 1:\n",
    "        tra_batch = unpickle(tra_file_2)\n",
    "    elif i == 2:\n",
    "        tra_batch = unpickle(tra_file_3)\n",
    "    elif i == 3:\n",
    "        tra_batch = unpickle(tra_file_4)\n",
    "    elif i == 4:\n",
    "        tra_batch = unpickle(tra_file_5)\n",
    "        \n",
    "    _X = tra_batch[\"data\"]\n",
    "    _Y = tra_batch['labels']\n",
    "\n",
    "    _X = np.array(_X, dtype=float) / 255.0\n",
    "    _X = _X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "    #_X = _X.reshape([-1, 3, 32, 32])\n",
    "    _X = _X.transpose([0, 2, 3, 1])\n",
    "    _X = _X.reshape(-1, 32*32*3)\n",
    "    _Y = np.array(_Y)\n",
    "\n",
    "    if x is None:\n",
    "        x = _X\n",
    "        y = _Y\n",
    "    else:\n",
    "        x = np.concatenate((x, _X), axis=0)\n",
    "        y = np.concatenate((y, _Y), axis=0)\n",
    "    \n",
    "    \n",
    "test_x = None \n",
    "test_y = None \n",
    "\n",
    "test_batch = unpickle(test_file)\n",
    "        \n",
    "_X = test_batch[\"data\"]\n",
    "_Y = test_batch['labels']\n",
    "\n",
    "_X = np.array(_X, dtype=float) / 255.0\n",
    "#_X = _X.reshape([-1, 3, 32, 32])\n",
    "_X = _X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "_X = _X.transpose([0, 2, 3, 1])\n",
    "_X = _X.reshape(-1, 32*32*3)\n",
    "\n",
    "\n",
    "x_test = _X\n",
    "y_test = _Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\" load single batch of cifar \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = './cifar-10-batches-py/'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_IMAGE_SIZE = 32\n",
    "_IMAGE_CHANNELS = 3\n",
    "_NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    X = tf.placeholder(dtype=tf.float32,shape=[None, _IMAGE_SIZE * _IMAGE_SIZE * _IMAGE_CHANNELS])\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n",
    "    \n",
    "    # hidden layer\n",
    "    W1 = var('W1',[_IMAGE_SIZE * _IMAGE_SIZE * _IMAGE_CHANNELS, 1000])\n",
    "    b1 = var('b1',[1000],tf.constant_initializer(0.1))\n",
    "    out1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    #output layer\n",
    "    W2 = var('W2',[1000, 10])\n",
    "    b2 = var('b2',[10],tf.constant_initializer(0.1))\n",
    "    logits = tf.matmul(out1, W2) + b2\n",
    "    \n",
    "    #accuracy\n",
    "    pred = tf.argmax(logits, axis=1)\n",
    "    truth = tf.argmax(Y, axis=1)\n",
    "    match = tf.cast(tf.equal(pred, truth), tf.float32)\n",
    "    acc = tf.reduce_mean(match)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "    \n",
    "    step = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(graph=g)\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 100\n",
    "for I in range(100):\n",
    "    _, acc_ = sess.run([step, acc], feed_dict={X:x, Y:y})\n",
    "    print('%d) acc: %2.4f' % (I,acc_), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define net\n",
    "class CifarNet():\n",
    "    def __init__(self):\n",
    "        # conv layer\n",
    "        # H2 = (H1 - F + 2P)/S +1\n",
    "        # (32-5)/1 + 1 = 28\n",
    "        # 28x28x32 = 25088\n",
    "        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n",
    "        self.Wconv1 = tf.get_variable(\"Wconv1\", shape=[5, 5, 3, 32])\n",
    "        self.bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "        # (32-5)/1 + 1 = 28\n",
    "        # 28x28x64 = 50176\n",
    "        self.Wconv2 = tf.get_variable(\"Wconv2\", shape=[5, 5, 32, 64])\n",
    "        self.bconv2 = tf.get_variable(\"bconv2\", shape=[64])\n",
    "        # affine layer with 1024\n",
    "        self.W1 = tf.get_variable(\"W1\", shape=[3136, 1024])\n",
    "        self.b1 = tf.get_variable(\"b1\", shape=[1024])\n",
    "        # affine layer with 10\n",
    "        self.W2 = tf.get_variable(\"W2\", shape=[1024, 10])\n",
    "        self.b2 = tf.get_variable(\"b2\", shape=[10])        \n",
    "        \n",
    "    def forward(self, X, y, is_training):\n",
    "        # conv2d\n",
    "        # ReLu\n",
    "        # conv2d\n",
    "        # ReLu\n",
    "        # maxpool\n",
    "        # Batch Norm\n",
    "        # Affine\n",
    "        # Batch Norm\n",
    "        # ReLu\n",
    "        # Affine\n",
    "        # dropout\n",
    "        # Batch Norm\n",
    "\n",
    "        # conv layer\n",
    "        # H2 = (H1 - F + 2P)/S +1\n",
    "        # (32-5)/1 + 1 = 28\n",
    "        # 28x28x32 = 25088\n",
    "        # To ReLu (?x16x16x32) -> MaxPool (?x16x16x32) -> affine (8192)\n",
    "\n",
    "        # define our graph (e.g. two_layer_convnet) with stride 1\n",
    "        conv1 = tf.nn.conv2d(X, self.Wconv1, strides=[1, 1, 1, 1], padding='SAME') + self.bconv1\n",
    "        print(conv1.shape)\n",
    "        # ReLU Activation Layer\n",
    "        relu1 = tf.nn.relu(conv1)\n",
    "        print(relu1)\n",
    "        # Conv\n",
    "        conv2 = tf.nn.conv2d(relu1, self.Wconv2, strides=[1, 2, 2, 1], padding='VALID') + self.bconv2\n",
    "        print(conv2.shape)\n",
    "        # ReLU Activation Layer\n",
    "        relu2 = tf.nn.relu(conv2)\n",
    "        print(relu2)\n",
    "        # 2x2 Max Pooling layer with a stride of 2\n",
    "        maxpool = tf.layers.max_pooling2d(relu2, pool_size=(2,2), strides=2)\n",
    "        print(maxpool.shape)\n",
    "        maxpool_flat = tf.reshape(maxpool,[-1,3136])\n",
    "        # Spatial Batch Normalization Layer (trainable parameters, with scale and centering)\n",
    "        bn1 = tf.layers.batch_normalization(inputs=maxpool_flat, center=True, scale=True, training=is_training)\n",
    "        # Affine layer with 1024 output units\n",
    "        affine1 = tf.matmul(bn1, self.W1) + self.b1\n",
    "        print(affine1.shape)\n",
    "        # vanilla batch normalization\n",
    "        affine1_flat = tf.reshape(affine1,[-1,1024])\n",
    "        bn2 = tf.layers.batch_normalization(inputs=affine1, center=True, scale=True, training=is_training)\n",
    "        print(bn2.shape)\n",
    "        # ReLU Activation Layer\n",
    "        relu2 = tf.nn.relu(bn2)\n",
    "        print(relu2.shape)\n",
    "        # dropout\n",
    "        drop1 = tf.layers.dropout(inputs=relu2, training=is_training)\n",
    "        # Affine layer from 1024 input units to 10 outputs\n",
    "        affine2 = tf.matmul(drop1, self.W2) + self.b2\n",
    "        # vanilla batch normalization\n",
    "        affine2_flat = tf.reshape(affine2,[-1,3136])\n",
    "        self.predict = tf.layers.batch_normalization(inputs=affine2, center=True, scale=True, training=is_training)\n",
    "        print(self.predict.shape)\n",
    "        return self.predict\n",
    "    \n",
    "    def run(self, session, loss_val, Xd, yd,\n",
    "                  epochs=1, batch_size=64, print_every=100,\n",
    "                  training=None, plot_losses=False, isSoftMax=False):\n",
    "        # have tensorflow compute accuracy\n",
    "        if isSoftMax:\n",
    "            correct_prediction = tf.nn.softmax(self.predict)\n",
    "        else:\n",
    "            correct_prediction = tf.equal(tf.argmax(self.predict,1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # shuffle indicies\n",
    "        train_indicies = np.arange(Xd.shape[0])\n",
    "        np.random.shuffle(train_indicies)\n",
    "\n",
    "        training_now = training is not None\n",
    "\n",
    "        # setting up variables we want to compute (and optimizing)\n",
    "        # if we have a training function, add that to things we compute\n",
    "        variables = [mean_loss, correct_prediction, accuracy]\n",
    "        if training_now:\n",
    "            variables[-1] = training\n",
    "\n",
    "        # counter \n",
    "        iter_cnt = 0\n",
    "        for e in range(epochs):\n",
    "            # keep track of losses and accuracy\n",
    "            correct = 0\n",
    "            losses = []\n",
    "            # make sure we iterate over the dataset once\n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                # generate indicies for the batch\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "\n",
    "                # create a feed dictionary for this batch\n",
    "                feed_dict = {X: Xd[idx,:],\n",
    "                             y: yd[idx],\n",
    "                             is_training: training_now }\n",
    "                # get batch size\n",
    "                actual_batch_size = yd[idx].shape[0]\n",
    "\n",
    "                # have tensorflow compute loss and correct predictions\n",
    "                # and (if given) perform a training step\n",
    "                loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "\n",
    "                # aggregate performance stats\n",
    "                losses.append(loss*actual_batch_size)\n",
    "                correct += np.sum(corr)\n",
    "\n",
    "                # print every now and then\n",
    "                if training_now and (iter_cnt % print_every) == 0:\n",
    "                    print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "                iter_cnt += 1\n",
    "            total_correct = correct/Xd.shape[0]\n",
    "            total_loss = np.sum(losses)/Xd.shape[0]\n",
    "            print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "                  .format(total_loss,total_correct,e+1))\n",
    "            if plot_losses:\n",
    "                plt.plot(losses)\n",
    "                plt.grid(True)\n",
    "                plt.title('Epoch {} Loss'.format(e+1))\n",
    "                plt.xlabel('minibatch number')\n",
    "                plt.ylabel('minibatch loss')\n",
    "                plt.show()\n",
    "        return total_loss, total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 32)\n",
      "Tensor(\"Relu:0\", shape=(?, 32, 32, 32), dtype=float32)\n",
      "(?, 14, 14, 64)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
      "(?, 7, 7, 64)\n",
      "(?, 1024)\n",
      "(?, 1024)\n",
      "(?, 1024)\n",
      "(?, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'batch_normalization_3/batchnorm/add_1:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "net = CifarNet()\n",
    "net.forward(X,y,is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-94b7eb5564d1>:20: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Annealing the learning rate\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1e-3\n",
    "end_learning_rate = 5e-3\n",
    "decay_steps = 10000\n",
    "\n",
    "learning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step,\n",
    "                                          decay_steps, end_learning_rate,\n",
    "                                          power=0.5)\n",
    "\n",
    "exp_learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               100000, 0.96, staircase=True)\n",
    "\n",
    "\n",
    "# Feel free to play with this cell\n",
    "mean_loss = None\n",
    "optimizer = None\n",
    "\n",
    "# define our loss\n",
    "cross_entr_loss = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y,10), logits=net.predict)\n",
    "mean_loss = tf.reduce_mean(cross_entr_loss)\n",
    "\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(exp_learning_rate)\n",
    "\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no gpu found, please use Google Cloud if you want GPU acceleration\n"
     ]
    }
   ],
   "source": [
    "# train with 10 epochs\n",
    "sess = tf.Session()\n",
    "\n",
    "try:\n",
    "    with tf.device(\"/gpu:0\") as dev:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Training')\n",
    "        net.run(sess, mean_loss, X_train, y_train, 10, 64, 200, train_step, True)\n",
    "        print('Validation')\n",
    "        net.run(sess, mean_loss, X_val, y_val, 1, 64)\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    print(\"no gpu found, please use Google Cloud if you want GPU acceleration\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
